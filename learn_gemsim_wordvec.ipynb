{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contemporary-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "changing-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer and machine applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "verbal-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coated-industry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'computer', 'machine'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "#stop_words = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stop_words]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "informal-privilege",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 17:52:56,517 : INFO : collecting all words and their counts\n",
      "2021-02-08 17:52:56,518 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-08 17:52:56,518 : INFO : collected 13 word types from a corpus of 31 raw words and 9 sentences\n",
      "2021-02-08 17:52:56,519 : INFO : Loading a fresh vocabulary\n",
      "2021-02-08 17:52:56,520 : INFO : effective_min_count=1 retains 13 unique words (100% of original 13, drops 0)\n",
      "2021-02-08 17:52:56,521 : INFO : effective_min_count=1 leaves 31 word corpus (100% of original 31, drops 0)\n",
      "2021-02-08 17:52:56,522 : INFO : deleting the raw counts dictionary of 13 items\n",
      "2021-02-08 17:52:56,522 : INFO : sample=0.001 downsamples 13 most-common words\n",
      "2021-02-08 17:52:56,523 : INFO : downsampling leaves estimated 3 word corpus (12.6% of prior 31)\n",
      "2021-02-08 17:52:56,523 : INFO : estimated required memory for 13 words and 100 dimensions: 16900 bytes\n",
      "2021-02-08 17:52:56,524 : INFO : resetting layer weights\n",
      "2021-02-08 17:52:56,530 : INFO : training model with 3 workers on 13 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-08 17:52:56,533 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 17:52:56,534 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 17:52:56,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 17:52:56,535 : INFO : EPOCH - 1 : training on 31 raw words (2 effective words) took 0.0s, 655 effective words/s\n",
      "2021-02-08 17:52:56,538 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 17:52:56,539 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 17:52:56,539 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 17:52:56,540 : INFO : EPOCH - 2 : training on 31 raw words (4 effective words) took 0.0s, 1858 effective words/s\n",
      "2021-02-08 17:52:56,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 17:52:56,543 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 17:52:56,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 17:52:56,544 : INFO : EPOCH - 3 : training on 31 raw words (1 effective words) took 0.0s, 486 effective words/s\n",
      "2021-02-08 17:52:56,546 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 17:52:56,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 17:52:56,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 17:52:56,549 : INFO : EPOCH - 4 : training on 31 raw words (5 effective words) took 0.0s, 1941 effective words/s\n",
      "2021-02-08 17:52:56,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 17:52:56,552 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 17:52:56,553 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 17:52:56,554 : INFO : EPOCH - 5 : training on 31 raw words (2 effective words) took 0.0s, 670 effective words/s\n",
      "2021-02-08 17:52:56,555 : INFO : training on a 155 raw words (14 effective words) took 0.0s, 564 effective words/s\n",
      "2021-02-08 17:52:56,556 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(processed_corpus, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adequate-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 18:02:00,455 : INFO : training model with 3 workers on 13 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-08 18:02:00,458 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:02:00,458 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:02:00,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:02:00,460 : INFO : EPOCH - 1 : training on 24 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-02-08 18:02:00,460 : WARNING : EPOCH - 1 : supplied example count (3) did not equal expected count (1)\n",
      "2021-02-08 18:02:00,463 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:02:00,464 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:02:00,465 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:02:00,465 : INFO : EPOCH - 2 : training on 24 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-02-08 18:02:00,466 : WARNING : EPOCH - 2 : supplied example count (3) did not equal expected count (1)\n",
      "2021-02-08 18:02:00,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:02:00,469 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:02:00,469 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:02:00,470 : INFO : EPOCH - 3 : training on 24 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-02-08 18:02:00,471 : WARNING : EPOCH - 3 : supplied example count (3) did not equal expected count (1)\n",
      "2021-02-08 18:02:00,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:02:00,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:02:00,475 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:02:00,476 : INFO : EPOCH - 4 : training on 24 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-02-08 18:02:00,477 : WARNING : EPOCH - 4 : supplied example count (3) did not equal expected count (1)\n",
      "2021-02-08 18:02:00,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:02:00,481 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:02:00,482 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:02:00,483 : INFO : EPOCH - 5 : training on 24 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-02-08 18:02:00,484 : WARNING : EPOCH - 5 : supplied example count (3) did not equal expected count (1)\n",
      "2021-02-08 18:02:00,484 : INFO : training on a 120 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2021-02-08 18:02:00,485 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 120)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model2 = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet\n",
    "#model2.build_vocab(processed_corpus)  # can be a non-repeatable, 1-pass generator\n",
    "new_doc = \"Human computer interaction\"\n",
    "model.train(new_doc.lower().split(),total_examples=1,epochs=5)  # can be a non-repeatable, 1-pass generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "muslim-neighbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 18:02:58,018 : INFO : Creating /Users/imayak/gensim-data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 100.0% 1662.2/1662.8MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 18:06:16,913 : INFO : word2vec-google-news-300 downloaded\n",
      "2021-02-08 18:06:16,916 : INFO : loading projection weights from /Users/imayak/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2021-02-08 18:07:00,943 : INFO : loaded (3000000, 300) matrix from /Users/imayak/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "checked-advancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index2word):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index2word)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "conservative-cincinnati",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king = wv['king']\n",
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "million-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "formed-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Efforting_soundbites', 0.6146546602249146), ('Watch_TimesCast_daily', 0.5961092114448547), ('teleconference', 0.5916281342506409), ('INCLUDES_previously_unreleased', 0.568611741065979), ('video_clips', 0.540194034576416), ('videos', 0.5204994678497314), ('www.bigtennetwork.com', 0.5163251757621765), ('audio', 0.5104407072067261), ('mms_:/', 0.5060652494430542), ('videotape', 0.5015642642974854)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['video', 'conference'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "induced-temperature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imayak/exper/python_nlp_venv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "supreme-sperm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 18:14:45,560 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-08 18:14:45,561 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "peaceful-thumb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 18:14:56,059 : INFO : collecting all words and their counts\n",
      "2021-02-08 18:14:56,064 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-08 18:14:56,158 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2021-02-08 18:14:56,159 : INFO : Loading a fresh vocabulary\n",
      "2021-02-08 18:14:56,165 : INFO : effective_min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "2021-02-08 18:14:56,166 : INFO : effective_min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "2021-02-08 18:14:56,172 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2021-02-08 18:14:56,173 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2021-02-08 18:14:56,173 : INFO : downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "2021-02-08 18:14:56,179 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
      "2021-02-08 18:14:56,180 : INFO : resetting layer weights\n",
      "2021-02-08 18:14:56,472 : INFO : training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-08 18:14:56,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:14:56,566 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:14:56,571 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:14:56,572 : INFO : EPOCH - 1 : training on 58152 raw words (35883 effective words) took 0.1s, 363892 effective words/s\n",
      "2021-02-08 18:14:56,666 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:14:56,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:14:56,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:14:56,673 : INFO : EPOCH - 2 : training on 58152 raw words (35909 effective words) took 0.1s, 383502 effective words/s\n",
      "2021-02-08 18:14:56,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:14:56,766 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:14:56,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:14:56,772 : INFO : EPOCH - 3 : training on 58152 raw words (36011 effective words) took 0.1s, 373336 effective words/s\n",
      "2021-02-08 18:14:56,860 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:14:56,861 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:14:56,867 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:14:56,867 : INFO : EPOCH - 4 : training on 58152 raw words (35955 effective words) took 0.1s, 384487 effective words/s\n",
      "2021-02-08 18:14:56,955 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 18:14:56,956 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 18:14:56,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 18:14:56,963 : INFO : EPOCH - 5 : training on 58152 raw words (35937 effective words) took 0.1s, 386992 effective words/s\n",
      "2021-02-08 18:14:56,963 : INFO : training on a 290760 raw words (179695 effective words) took 0.5s, 366246 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "informational-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 19:27:51,407 : INFO : saving Word2Vec object under /var/folders/58/6ywxrmgj7mv2t7lhy0t2h7bh00r92n/T/gensim-model-j2lysxmm, separately None\n",
      "2021-02-08 19:27:51,408 : INFO : not storing attribute vectors_norm\n",
      "2021-02-08 19:27:51,409 : INFO : not storing attribute cum_table\n",
      "2021-02-08 19:27:51,422 : INFO : saved /var/folders/58/6ywxrmgj7mv2t7lhy0t2h7bh00r92n/T/gensim-model-j2lysxmm\n",
      "2021-02-08 19:27:51,423 : INFO : loading Word2Vec object from /var/folders/58/6ywxrmgj7mv2t7lhy0t2h7bh00r92n/T/gensim-model-j2lysxmm\n",
      "2021-02-08 19:27:51,440 : INFO : loading wv recursively from /var/folders/58/6ywxrmgj7mv2t7lhy0t2h7bh00r92n/T/gensim-model-j2lysxmm.wv.* with mmap=None\n",
      "2021-02-08 19:27:51,440 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-02-08 19:27:51,441 : INFO : loading vocabulary recursively from /var/folders/58/6ywxrmgj7mv2t7lhy0t2h7bh00r92n/T/gensim-model-j2lysxmm.vocabulary.* with mmap=None\n",
      "2021-02-08 19:27:51,442 : INFO : loading trainables recursively from /var/folders/58/6ywxrmgj7mv2t7lhy0t2h7bh00r92n/T/gensim-model-j2lysxmm.trainables.* with mmap=None\n",
      "2021-02-08 19:27:51,443 : INFO : setting ignored attribute cum_table to None\n",
      "2021-02-08 19:27:51,444 : INFO : loaded /var/folders/58/6ywxrmgj7mv2t7lhy0t2h7bh00r92n/T/gensim-model-j2lysxmm\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "considered-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-08 19:28:07,747 : INFO : collecting all words and their counts\n",
      "2021-02-08 19:28:07,750 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-08 19:28:07,750 : INFO : collected 13 word types from a corpus of 13 raw words and 1 sentences\n",
      "2021-02-08 19:28:07,751 : INFO : Updating model with new vocabulary\n",
      "2021-02-08 19:28:07,752 : INFO : New added 0 unique words (0% of original 13) and increased the count of 0 pre-existing words (0% of original 13)\n",
      "2021-02-08 19:28:07,752 : INFO : deleting the raw counts dictionary of 13 items\n",
      "2021-02-08 19:28:07,753 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2021-02-08 19:28:07,753 : INFO : downsampling leaves estimated 0 word corpus (0.0% of prior 0)\n",
      "2021-02-08 19:28:07,756 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
      "2021-02-08 19:28:07,757 : INFO : updating layer weights\n",
      "2021-02-08 19:28:07,758 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2021-02-08 19:28:07,759 : INFO : training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-08 19:28:07,762 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 19:28:07,763 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 19:28:07,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 19:28:07,764 : INFO : EPOCH - 1 : training on 13 raw words (6 effective words) took 0.0s, 2254 effective words/s\n",
      "2021-02-08 19:28:07,765 : WARNING : EPOCH - 1 : supplied example count (1) did not equal expected count (300)\n",
      "2021-02-08 19:28:07,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 19:28:07,769 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 19:28:07,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 19:28:07,770 : INFO : EPOCH - 2 : training on 13 raw words (5 effective words) took 0.0s, 1805 effective words/s\n",
      "2021-02-08 19:28:07,771 : WARNING : EPOCH - 2 : supplied example count (1) did not equal expected count (300)\n",
      "2021-02-08 19:28:07,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 19:28:07,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 19:28:07,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 19:28:07,775 : INFO : EPOCH - 3 : training on 13 raw words (5 effective words) took 0.0s, 3515 effective words/s\n",
      "2021-02-08 19:28:07,775 : WARNING : EPOCH - 3 : supplied example count (1) did not equal expected count (300)\n",
      "2021-02-08 19:28:07,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 19:28:07,779 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 19:28:07,779 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 19:28:07,780 : INFO : EPOCH - 4 : training on 13 raw words (6 effective words) took 0.0s, 2147 effective words/s\n",
      "2021-02-08 19:28:07,781 : WARNING : EPOCH - 4 : supplied example count (1) did not equal expected count (300)\n",
      "2021-02-08 19:28:07,784 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-08 19:28:07,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-08 19:28:07,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-08 19:28:07,787 : INFO : EPOCH - 5 : training on 13 raw words (5 effective words) took 0.0s, 1884 effective words/s\n",
      "2021-02-08 19:28:07,787 : WARNING : EPOCH - 5 : supplied example count (1) did not equal expected count (300)\n",
      "2021-02-08 19:28:07,789 : INFO : training on a 65 raw words (27 effective words) took 0.0s, 913 effective words/s\n",
      "2021-02-08 19:28:07,790 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n",
    "]\n",
    "new_model.build_vocab(more_sentences, update=True)\n",
    "new_model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# cleaning up temporary file\n",
    "import os\n",
    "os.remove(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "essential-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b4d9c6893336>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  new_model.most_similar(positive=['training'], topn=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('at', 0.999830961227417),\n",
       " ('minister', 0.9998297691345215),\n",
       " ('were', 0.9998275637626648),\n",
       " ('out', 0.9998210668563843),\n",
       " ('which', 0.9998193383216858),\n",
       " ('other', 0.9998165369033813),\n",
       " ('or', 0.9998159408569336),\n",
       " ('those', 0.9998151063919067),\n",
       " ('several', 0.9998124837875366),\n",
       " ('when', 0.9998108744621277)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.most_similar(positive=['training'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acceptable-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp38-cp38-macosx_10_13_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /Users/imayak/exper/python_nlp_venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.20.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/imayak/exper/python_nlp_venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/imayak/exper/python_nlp_venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "    Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed scikit-learn-0.24.1 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dental-firewall",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-8d0de53a56ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_with_plotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mplot_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-8d0de53a56ce>\u001b[0m in \u001b[0;36mplot_with_plotly\u001b[0;34m(x_vals, y_vals, labels, plot_in_notebook)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_with_plotly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_in_notebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_notebook_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index2word)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(new_model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-somerset",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_nlp_venv",
   "language": "python",
   "name": "python_nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
