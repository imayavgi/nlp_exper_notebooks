{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arabic-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "saved-moore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-douglas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "#print(dictionary.token2id)\n",
    "dictionary.doc2bow('Preparing report'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scientific-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "maritime-wireless",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(42, 1), (60, 1), (61, 1), (62, 1)], [(62, 4)]]\n"
     ]
    }
   ],
   "source": [
    "my_docs = [\"Who let  dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "increasing-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bored-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(mycorpus, smartirs='ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sudden-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = api.load(\"text8\")\n",
    "dataset = [wd for wd in data]\n",
    "\n",
    "dct = corpora.Dictionary(dataset)\n",
    "corpus = [dct.doc2bow(line) for line in dataset]\n",
    "\n",
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(dataset, min_count=3, threshold=10)\n",
    "\n",
    "print(bigram[dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-lying",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the trigram models\n",
    "trigram = gensim.models.phrases.Phrases(bigram[dataset], threshold=10)\n",
    "\n",
    "# Construct trigram\n",
    "print(trigram[bigram[dataset[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+git://github.com/pattern3/pattern.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "twenty-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['com', 'also','edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "liquid-plymouth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'term', 'abuse', 'first']\n"
     ]
    }
   ],
   "source": [
    "data = api.load(\"text8\")\n",
    "dataset = [wd for wd in data]\n",
    "data_processed = []\n",
    "for i, doc in enumerate(dataset[:50]):\n",
    "    doc_out = []\n",
    "    for wd in doc:\n",
    "        if wd not in stop_words:  # remove stopwords\n",
    "            lemmatized_word = lemmatize(wd, allowed_tags=re.compile('(NN|JJ|RB)'))  # lemmatize\n",
    "            if lemmatized_word:\n",
    "                doc_out = doc_out + [lemmatized_word[0].split(b'/')[0].decode('utf-8')]\n",
    "        else:\n",
    "            continue\n",
    "    data_processed.append(doc_out)\n",
    "\n",
    "# Print a small sample    \n",
    "print(data_processed[0][:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "novel-ballet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:33:49,437 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-10 12:33:49,682 : INFO : built Dictionary(24520 unique tokens: ['ability', 'able', 'abnormal', 'abolition', 'absence']...) from 50 documents (total 215171 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dct = corpora.Dictionary(data_processed)\n",
    "corpus = [dct.doc2bow(line) for line in data_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "liable-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:33:52,427 : INFO : using asymmetric alpha [0.26219156, 0.19027454, 0.14931786, 0.12287004, 0.104381524, 0.090729296, 0.080235206]\n",
      "2021-02-10 12:33:52,428 : INFO : using symmetric eta at 0.14285714285714285\n",
      "2021-02-10 12:33:52,434 : INFO : using serial LDA version on this node\n",
      "2021-02-10 12:33:52,454 : INFO : running online LDA training, 7 topics, 10 passes over the supplied corpus of 50 documents, updating every 11000 documents, evaluating every ~0 documents, iterating 100x with a convergence threshold of 0.001000\n",
      "2021-02-10 12:33:52,455 : INFO : training LDA model using 11 processes\n",
      "2021-02-10 12:33:52,675 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:54,560 : INFO : topic #6 (0.080): 0.001*\"first\" + 0.001*\"many\" + 0.001*\"god\" + 0.001*\"state\" + 0.000*\"world\" + 0.000*\"apollo\" + 0.000*\"person\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"new\"\n",
      "2021-02-10 12:33:54,562 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"film\" + 0.000*\"war\" + 0.000*\"often\"\n",
      "2021-02-10 12:33:54,564 : INFO : topic #2 (0.149): 0.001*\"state\" + 0.001*\"many\" + 0.001*\"first\" + 0.000*\"atom\" + 0.000*\"name\" + 0.000*\"however\" + 0.000*\"time\" + 0.000*\"country\" + 0.000*\"person\" + 0.000*\"number\"\n",
      "2021-02-10 12:33:54,566 : INFO : topic #1 (0.190): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"agave\" + 0.001*\"american\" + 0.001*\"war\" + 0.001*\"new\" + 0.001*\"time\" + 0.001*\"year\" + 0.000*\"first\" + 0.000*\"person\"\n",
      "2021-02-10 12:33:54,567 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:54,568 : INFO : topic diff=0.202682, rho=0.125000\n",
      "2021-02-10 12:33:54,570 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:55,038 : INFO : topic #6 (0.080): 0.001*\"god\" + 0.001*\"atheism\" + 0.001*\"apollo\" + 0.001*\"first\" + 0.001*\"atheist\" + 0.001*\"many\" + 0.001*\"world\" + 0.001*\"open\" + 0.001*\"agassi\" + 0.001*\"state\"\n",
      "2021-02-10 12:33:55,040 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:55,041 : INFO : topic #2 (0.149): 0.001*\"atom\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"many\" + 0.001*\"element\" + 0.001*\"number\" + 0.000*\"electron\" + 0.000*\"however\" + 0.000*\"name\" + 0.000*\"arsenic\"\n",
      "2021-02-10 12:33:55,042 : INFO : topic #1 (0.190): 0.001*\"american\" + 0.001*\"football\" + 0.001*\"many\" + 0.001*\"player\" + 0.001*\"war\" + 0.001*\"play\" + 0.001*\"state\" + 0.001*\"einstein\" + 0.001*\"new\" + 0.001*\"ball\"\n",
      "2021-02-10 12:33:55,043 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:55,045 : INFO : topic diff=0.146239, rho=0.123987\n",
      "2021-02-10 12:33:55,046 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:55,267 : INFO : topic #6 (0.080): 0.002*\"atheism\" + 0.002*\"apollo\" + 0.002*\"god\" + 0.001*\"atheist\" + 0.001*\"first\" + 0.001*\"agassi\" + 0.001*\"open\" + 0.001*\"moon\" + 0.001*\"many\" + 0.001*\"world\"\n",
      "2021-02-10 12:33:55,268 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:55,270 : INFO : topic #2 (0.149): 0.001*\"atom\" + 0.001*\"electron\" + 0.001*\"arsenic\" + 0.001*\"element\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"many\" + 0.001*\"number\" + 0.000*\"antimony\" + 0.000*\"however\"\n",
      "2021-02-10 12:33:55,271 : INFO : topic #1 (0.190): 0.002*\"american\" + 0.002*\"football\" + 0.002*\"player\" + 0.002*\"play\" + 0.002*\"ball\" + 0.001*\"line\" + 0.001*\"war\" + 0.001*\"team\" + 0.001*\"british\" + 0.001*\"many\"\n",
      "2021-02-10 12:33:55,272 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:55,273 : INFO : topic diff=0.111052, rho=0.123045\n",
      "2021-02-10 12:33:55,274 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:55,454 : INFO : topic #6 (0.080): 0.003*\"atheism\" + 0.003*\"apollo\" + 0.003*\"god\" + 0.002*\"atheist\" + 0.002*\"agassi\" + 0.001*\"open\" + 0.001*\"first\" + 0.001*\"moon\" + 0.001*\"crew\" + 0.001*\"final\"\n",
      "2021-02-10 12:33:55,455 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:55,457 : INFO : topic #2 (0.149): 0.002*\"atom\" + 0.001*\"arsenic\" + 0.001*\"electron\" + 0.001*\"element\" + 0.001*\"antimony\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"number\" + 0.001*\"many\" + 0.001*\"atomic\"\n",
      "2021-02-10 12:33:55,458 : INFO : topic #1 (0.190): 0.003*\"football\" + 0.003*\"player\" + 0.002*\"american\" + 0.002*\"ball\" + 0.002*\"play\" + 0.002*\"line\" + 0.002*\"team\" + 0.002*\"war\" + 0.002*\"british\" + 0.001*\"many\"\n",
      "2021-02-10 12:33:55,459 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:55,460 : INFO : topic diff=0.105210, rho=0.122124\n",
      "2021-02-10 12:33:55,461 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:55,630 : INFO : topic #6 (0.080): 0.004*\"atheism\" + 0.004*\"apollo\" + 0.003*\"god\" + 0.002*\"atheist\" + 0.002*\"agassi\" + 0.002*\"open\" + 0.001*\"first\" + 0.001*\"moon\" + 0.001*\"crew\" + 0.001*\"final\"\n",
      "2021-02-10 12:33:55,631 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:55,632 : INFO : topic #2 (0.149): 0.002*\"atom\" + 0.001*\"arsenic\" + 0.001*\"electron\" + 0.001*\"element\" + 0.001*\"antimony\" + 0.001*\"atomic\" + 0.001*\"number\" + 0.001*\"state\" + 0.001*\"compound\" + 0.001*\"first\"\n",
      "2021-02-10 12:33:55,633 : INFO : topic #1 (0.190): 0.003*\"football\" + 0.003*\"player\" + 0.003*\"american\" + 0.003*\"ball\" + 0.003*\"play\" + 0.002*\"line\" + 0.002*\"team\" + 0.002*\"british\" + 0.002*\"war\" + 0.001*\"back\"\n",
      "2021-02-10 12:33:55,634 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:55,635 : INFO : topic diff=0.107223, rho=0.121223\n",
      "2021-02-10 12:33:55,636 : INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:55,805 : INFO : topic #6 (0.080): 0.005*\"atheism\" + 0.004*\"apollo\" + 0.004*\"god\" + 0.003*\"atheist\" + 0.003*\"agassi\" + 0.002*\"open\" + 0.002*\"moon\" + 0.001*\"crew\" + 0.001*\"first\" + 0.001*\"final\"\n",
      "2021-02-10 12:33:55,806 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:55,808 : INFO : topic #2 (0.149): 0.003*\"atom\" + 0.002*\"arsenic\" + 0.002*\"electron\" + 0.001*\"element\" + 0.001*\"antimony\" + 0.001*\"atomic\" + 0.001*\"compound\" + 0.001*\"isotope\" + 0.001*\"neutron\" + 0.001*\"number\"\n",
      "2021-02-10 12:33:55,809 : INFO : topic #1 (0.190): 0.004*\"football\" + 0.004*\"player\" + 0.004*\"ball\" + 0.003*\"american\" + 0.003*\"play\" + 0.003*\"line\" + 0.003*\"team\" + 0.002*\"british\" + 0.002*\"war\" + 0.002*\"offensive\"\n",
      "2021-02-10 12:33:55,810 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:55,811 : INFO : topic diff=0.112088, rho=0.120342\n",
      "2021-02-10 12:33:55,812 : INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:33:55,989 : INFO : topic #6 (0.080): 0.006*\"atheism\" + 0.005*\"apollo\" + 0.004*\"god\" + 0.004*\"atheist\" + 0.003*\"agassi\" + 0.003*\"open\" + 0.002*\"moon\" + 0.002*\"crew\" + 0.002*\"final\" + 0.002*\"lunar\"\n",
      "2021-02-10 12:33:55,990 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:55,991 : INFO : topic #2 (0.149): 0.003*\"atom\" + 0.002*\"electron\" + 0.002*\"arsenic\" + 0.002*\"element\" + 0.002*\"antimony\" + 0.001*\"atomic\" + 0.001*\"compound\" + 0.001*\"isotope\" + 0.001*\"neutron\" + 0.001*\"argon\"\n",
      "2021-02-10 12:33:55,993 : INFO : topic #1 (0.190): 0.005*\"football\" + 0.005*\"player\" + 0.004*\"ball\" + 0.004*\"american\" + 0.004*\"play\" + 0.003*\"line\" + 0.003*\"team\" + 0.003*\"british\" + 0.002*\"war\" + 0.002*\"offensive\"\n",
      "2021-02-10 12:33:55,994 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:55,995 : INFO : topic diff=0.118117, rho=0.119480\n",
      "2021-02-10 12:33:55,996 : INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:56,189 : INFO : topic #6 (0.080): 0.008*\"atheism\" + 0.006*\"apollo\" + 0.005*\"god\" + 0.004*\"atheist\" + 0.003*\"agassi\" + 0.003*\"open\" + 0.002*\"moon\" + 0.002*\"crew\" + 0.002*\"final\" + 0.002*\"lunar\"\n",
      "2021-02-10 12:33:56,191 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:56,192 : INFO : topic #2 (0.149): 0.004*\"atom\" + 0.003*\"electron\" + 0.002*\"arsenic\" + 0.002*\"element\" + 0.002*\"antimony\" + 0.001*\"atomic\" + 0.001*\"compound\" + 0.001*\"isotope\" + 0.001*\"neutron\" + 0.001*\"nucleus\"\n",
      "2021-02-10 12:33:56,193 : INFO : topic #1 (0.190): 0.006*\"football\" + 0.005*\"player\" + 0.005*\"ball\" + 0.005*\"american\" + 0.005*\"play\" + 0.004*\"line\" + 0.004*\"team\" + 0.003*\"british\" + 0.003*\"war\" + 0.002*\"back\"\n",
      "2021-02-10 12:33:56,194 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:56,195 : INFO : topic diff=0.124539, rho=0.118636\n",
      "2021-02-10 12:33:56,196 : INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:56,365 : INFO : topic #6 (0.080): 0.009*\"atheism\" + 0.007*\"apollo\" + 0.006*\"god\" + 0.005*\"atheist\" + 0.004*\"agassi\" + 0.003*\"open\" + 0.002*\"moon\" + 0.002*\"crew\" + 0.002*\"final\" + 0.002*\"lunar\"\n",
      "2021-02-10 12:33:56,366 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.000*\"first\" + 0.000*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:56,368 : INFO : topic #2 (0.149): 0.005*\"atom\" + 0.003*\"electron\" + 0.003*\"arsenic\" + 0.002*\"element\" + 0.002*\"antimony\" + 0.002*\"atomic\" + 0.001*\"compound\" + 0.001*\"isotope\" + 0.001*\"neutron\" + 0.001*\"nucleus\"\n",
      "2021-02-10 12:33:56,369 : INFO : topic #1 (0.190): 0.006*\"football\" + 0.006*\"player\" + 0.005*\"ball\" + 0.005*\"play\" + 0.005*\"american\" + 0.004*\"line\" + 0.004*\"team\" + 0.003*\"british\" + 0.003*\"war\" + 0.003*\"back\"\n",
      "2021-02-10 12:33:56,370 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:56,371 : INFO : topic diff=0.130917, rho=0.117810\n",
      "2021-02-10 12:33:56,372 : INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #50/50, outstanding queue size 1\n",
      "2021-02-10 12:33:56,550 : INFO : topic #6 (0.080): 0.010*\"atheism\" + 0.007*\"apollo\" + 0.007*\"god\" + 0.005*\"atheist\" + 0.004*\"agassi\" + 0.004*\"open\" + 0.003*\"moon\" + 0.002*\"crew\" + 0.002*\"final\" + 0.002*\"lunar\"\n",
      "2021-02-10 12:33:56,551 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.000*\"first\" + 0.000*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:56,552 : INFO : topic #2 (0.149): 0.006*\"atom\" + 0.003*\"electron\" + 0.003*\"arsenic\" + 0.003*\"element\" + 0.002*\"antimony\" + 0.002*\"atomic\" + 0.002*\"compound\" + 0.001*\"isotope\" + 0.001*\"neutron\" + 0.001*\"nucleus\"\n",
      "2021-02-10 12:33:56,554 : INFO : topic #1 (0.190): 0.007*\"football\" + 0.006*\"player\" + 0.006*\"ball\" + 0.006*\"play\" + 0.006*\"american\" + 0.005*\"line\" + 0.004*\"team\" + 0.004*\"british\" + 0.003*\"war\" + 0.003*\"back\"\n",
      "2021-02-10 12:33:56,555 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:56,556 : INFO : topic diff=0.136934, rho=0.117001\n",
      "2021-02-10 12:33:56,595 : INFO : saving LdaState object under lda_model.model.state, separately None\n",
      "2021-02-10 12:33:56,602 : INFO : saved lda_model.model.state\n",
      "2021-02-10 12:33:56,631 : INFO : saving LdaMulticore object under lda_model.model, separately ['expElogbeta', 'sstats']\n",
      "2021-02-10 12:33:56,631 : INFO : storing np array 'expElogbeta' to lda_model.model.expElogbeta.npy\n",
      "2021-02-10 12:33:56,634 : INFO : not storing attribute id2word\n",
      "2021-02-10 12:33:56,635 : INFO : not storing attribute dispatcher\n",
      "2021-02-10 12:33:56,636 : INFO : not storing attribute state\n",
      "2021-02-10 12:33:56,638 : INFO : saved lda_model.model\n",
      "2021-02-10 12:33:56,639 : INFO : topic #0 (0.262): 0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"\n",
      "2021-02-10 12:33:56,641 : INFO : topic #1 (0.190): 0.007*\"football\" + 0.006*\"player\" + 0.006*\"ball\" + 0.006*\"play\" + 0.006*\"american\" + 0.005*\"line\" + 0.004*\"team\" + 0.004*\"british\" + 0.003*\"war\" + 0.003*\"back\"\n",
      "2021-02-10 12:33:56,642 : INFO : topic #2 (0.149): 0.006*\"atom\" + 0.003*\"electron\" + 0.003*\"arsenic\" + 0.003*\"element\" + 0.002*\"antimony\" + 0.002*\"atomic\" + 0.002*\"compound\" + 0.001*\"isotope\" + 0.001*\"neutron\" + 0.001*\"nucleus\"\n",
      "2021-02-10 12:33:56,644 : INFO : topic #3 (0.123): 0.005*\"state\" + 0.004*\"many\" + 0.004*\"first\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"world\" + 0.003*\"year\" + 0.003*\"new\" + 0.003*\"war\" + 0.003*\"agave\"\n",
      "2021-02-10 12:33:56,645 : INFO : topic #4 (0.104): 0.008*\"audi\" + 0.005*\"car\" + 0.005*\"engine\" + 0.003*\"vehicle\" + 0.002*\"automobile\" + 0.002*\"aircraft\" + 0.002*\"wheel\" + 0.001*\"first\" + 0.001*\"drive\" + 0.001*\"quattro\"\n",
      "2021-02-10 12:33:56,646 : INFO : topic #5 (0.091): 0.001*\"many\" + 0.001*\"state\" + 0.000*\"first\" + 0.000*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"\n",
      "2021-02-10 12:33:56,647 : INFO : topic #6 (0.080): 0.010*\"atheism\" + 0.007*\"apollo\" + 0.007*\"god\" + 0.005*\"atheist\" + 0.004*\"agassi\" + 0.004*\"open\" + 0.003*\"moon\" + 0.002*\"crew\" + 0.002*\"final\" + 0.002*\"lunar\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"state\" + 0.000*\"world\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"person\" + 0.000*\"war\" + 0.000*\"year\" + 0.000*\"atheism\" + 0.000*\"american\" + 0.000*\"time\"'),\n",
       " (1,\n",
       "  '0.007*\"football\" + 0.006*\"player\" + 0.006*\"ball\" + 0.006*\"play\" + 0.006*\"american\" + 0.005*\"line\" + 0.004*\"team\" + 0.004*\"british\" + 0.003*\"war\" + 0.003*\"back\"'),\n",
       " (2,\n",
       "  '0.006*\"atom\" + 0.003*\"electron\" + 0.003*\"arsenic\" + 0.003*\"element\" + 0.002*\"antimony\" + 0.002*\"atomic\" + 0.002*\"compound\" + 0.001*\"isotope\" + 0.001*\"neutron\" + 0.001*\"nucleus\"'),\n",
       " (3,\n",
       "  '0.005*\"state\" + 0.004*\"many\" + 0.004*\"first\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"world\" + 0.003*\"year\" + 0.003*\"new\" + 0.003*\"war\" + 0.003*\"agave\"'),\n",
       " (4,\n",
       "  '0.008*\"audi\" + 0.005*\"car\" + 0.005*\"engine\" + 0.003*\"vehicle\" + 0.002*\"automobile\" + 0.002*\"aircraft\" + 0.002*\"wheel\" + 0.001*\"first\" + 0.001*\"drive\" + 0.001*\"quattro\"'),\n",
       " (5,\n",
       "  '0.001*\"many\" + 0.001*\"state\" + 0.000*\"first\" + 0.000*\"acid\" + 0.000*\"time\" + 0.000*\"person\" + 0.000*\"year\" + 0.000*\"anchorage\" + 0.000*\"american\" + 0.000*\"alaska\"'),\n",
       " (6,\n",
       "  '0.010*\"atheism\" + 0.007*\"apollo\" + 0.007*\"god\" + 0.005*\"atheist\" + 0.004*\"agassi\" + 0.004*\"open\" + 0.003*\"moon\" + 0.002*\"crew\" + 0.002*\"final\" + 0.002*\"lunar\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=7,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "understood-killing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Topics      :  [(3, 0.9997827)]\n",
      "Word id, Topics      :  [(0, [3]), (7, [3]), (10, [3])]\n",
      "Phi Values (word id) :  [(0, [(3, 2.9987354)]), (7, [(3, 0.9869535)])]\n",
      "Word, Topics         :  [('ability', [3]), ('absurdity', [3])]\n",
      "Phi Values (word)    :  [('ability', [(3, 2.9987354)]), ('absurdity', [(3, 0.9869535)])]\n",
      "------------------------------------------------------\n",
      "\n",
      "Document Topics      :  [(3, 0.99978393)]\n",
      "Word id, Topics      :  [(0, [3]), (10, [3]), (16, [3])]\n",
      "Phi Values (word id) :  [(0, [(3, 5.997471)]), (10, [(3, 2.9968653)])]\n",
      "Word, Topics         :  [('ability', [3]), ('academic', [3])]\n",
      "Phi Values (word)    :  [('ability', [(3, 5.997471)]), ('academic', [(3, 2.9968653)])]\n",
      "------------------------------------------------------\n",
      "\n",
      "Document Topics      :  [(3, 0.9998095)]\n",
      "Word id, Topics      :  [(1, [3]), (10, [3]), (15, [3])]\n",
      "Phi Values (word id) :  [(1, [(3, 0.9996602)]), (10, [(3, 5.9937315)])]\n",
      "Word, Topics         :  [('able', [3]), ('academic', [3])]\n",
      "Phi Values (word)    :  [('able', [(3, 0.9996602)]), ('academic', [(3, 5.9937315)])]\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in lda_model[corpus[5:8]]:\n",
    "    print(\"Document Topics      : \", c[0])      # [(Topics, Perc Contrib)]\n",
    "    print(\"Word id, Topics      : \", c[1][:3])  # [(Word id, [Topics])]\n",
    "    print(\"Phi Values (word id) : \", c[2][:2])  # [(Word id, [(Topic, Phi Value)])]\n",
    "    print(\"Word, Topics         : \", [(dct[wd], topic) for wd, topic in c[1][:2]])   # [(Word, [Topics])]\n",
    "    print(\"Phi Values (word)    : \", [(dct[wd], topic) for wd, topic in c[2][:2]])  # [(Word, [(Topic, Phi Value)])]\n",
    "    print(\"------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "studied-clone",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:35:53,581 : INFO : using serial LSI version on this node\n",
      "2021-02-10 12:35:53,582 : INFO : updating model with new documents\n",
      "2021-02-10 12:35:53,583 : INFO : preparing a new chunk of documents\n",
      "2021-02-10 12:35:53,603 : INFO : using 100 extra samples and 2 power iterations\n",
      "2021-02-10 12:35:53,604 : INFO : 1st phase: constructing (24520, 107) action matrix\n",
      "2021-02-10 12:35:53,628 : INFO : orthonormalizing (24520, 107) action matrix\n",
      "2021-02-10 12:35:54,211 : INFO : 2nd phase: running dense svd on (107, 50) matrix\n",
      "2021-02-10 12:35:54,228 : INFO : computing the final decomposition\n",
      "2021-02-10 12:35:54,230 : INFO : keeping 7 factors (discarding 55.737% of energy spectrum)\n",
      "2021-02-10 12:35:54,242 : INFO : processed documents up to #50\n",
      "2021-02-10 12:35:54,245 : INFO : topic #0(713.655): 0.289*\"agave\" + 0.219*\"state\" + 0.159*\"many\" + 0.154*\"first\" + 0.136*\"person\" + 0.135*\"time\" + 0.132*\"year\" + 0.131*\"war\" + 0.128*\"world\" + 0.124*\"american\"\n",
      "2021-02-10 12:35:54,247 : INFO : topic #1(560.938): -0.898*\"agave\" + -0.152*\"asia\" + -0.096*\"aruba\" + 0.069*\"first\" + 0.062*\"time\" + 0.061*\"many\" + 0.060*\"person\" + -0.054*\"plant\" + -0.051*\"var\" + 0.050*\"war\"\n",
      "2021-02-10 12:35:54,249 : INFO : topic #2(347.013): 0.296*\"football\" + 0.252*\"player\" + 0.251*\"american\" + 0.228*\"ball\" + 0.220*\"war\" + 0.209*\"play\" + 0.177*\"line\" + 0.172*\"team\" + 0.162*\"lincoln\" + -0.160*\"atheism\"\n",
      "2021-02-10 12:35:54,251 : INFO : topic #3(315.706): 0.546*\"lincoln\" + 0.194*\"state\" + -0.187*\"football\" + 0.181*\"aristotle\" + -0.181*\"player\" + 0.177*\"union\" + -0.140*\"ball\" + 0.140*\"war\" + -0.119*\"play\" + 0.115*\"achille\"\n",
      "2021-02-10 12:35:54,252 : INFO : topic #4(310.675): -0.534*\"atheism\" + -0.373*\"god\" + -0.316*\"atheist\" + -0.176*\"lincoln\" + -0.154*\"belief\" + 0.127*\"africa\" + -0.122*\"existence\" + -0.117*\"religion\" + -0.102*\"deity\" + 0.102*\"hiv\"\n",
      "2021-02-10 12:35:54,254 : INFO : topic #0(713.655): 0.289*\"agave\" + 0.219*\"state\" + 0.159*\"many\" + 0.154*\"first\" + 0.136*\"person\" + 0.135*\"time\" + 0.132*\"year\" + 0.131*\"war\" + 0.128*\"world\" + 0.124*\"american\"\n",
      "2021-02-10 12:35:54,256 : INFO : topic #1(560.938): -0.898*\"agave\" + -0.152*\"asia\" + -0.096*\"aruba\" + 0.069*\"first\" + 0.062*\"time\" + 0.061*\"many\" + 0.060*\"person\" + -0.054*\"plant\" + -0.051*\"var\" + 0.050*\"war\"\n",
      "2021-02-10 12:35:54,258 : INFO : topic #2(347.013): 0.296*\"football\" + 0.252*\"player\" + 0.251*\"american\" + 0.228*\"ball\" + 0.220*\"war\" + 0.209*\"play\" + 0.177*\"line\" + 0.172*\"team\" + 0.162*\"lincoln\" + -0.160*\"atheism\"\n",
      "2021-02-10 12:35:54,259 : INFO : topic #3(315.706): 0.546*\"lincoln\" + 0.194*\"state\" + -0.187*\"football\" + 0.181*\"aristotle\" + -0.181*\"player\" + 0.177*\"union\" + -0.140*\"ball\" + 0.140*\"war\" + -0.119*\"play\" + 0.115*\"achille\"\n",
      "2021-02-10 12:35:54,262 : INFO : topic #4(310.675): -0.534*\"atheism\" + -0.373*\"god\" + -0.316*\"atheist\" + -0.176*\"lincoln\" + -0.154*\"belief\" + 0.127*\"africa\" + -0.122*\"existence\" + -0.117*\"religion\" + -0.102*\"deity\" + 0.102*\"hiv\"\n",
      "2021-02-10 12:35:54,263 : INFO : topic #5(289.010): 0.286*\"lincoln\" + -0.222*\"aluminium\" + -0.206*\"island\" + 0.194*\"hiv\" + 0.176*\"abortion\" + -0.171*\"apple\" + 0.169*\"film\" + 0.159*\"aristotle\" + -0.152*\"australia\" + -0.130*\"church\"\n",
      "2021-02-10 12:35:54,265 : INFO : topic #6(281.559): 0.363*\"atom\" + 0.257*\"alkane\" + -0.201*\"hiv\" + 0.170*\"lincoln\" + 0.167*\"element\" + 0.163*\"aluminium\" + 0.159*\"electron\" + 0.156*\"court\" + 0.154*\"carbon\" + -0.143*\"africa\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.289*\"agave\" + 0.219*\"state\" + 0.159*\"many\" + 0.154*\"first\" + '\n",
      "  '0.136*\"person\" + 0.135*\"time\" + 0.132*\"year\" + 0.131*\"war\" + 0.128*\"world\" '\n",
      "  '+ 0.124*\"american\"'),\n",
      " (1,\n",
      "  '-0.898*\"agave\" + -0.152*\"asia\" + -0.096*\"aruba\" + 0.069*\"first\" + '\n",
      "  '0.062*\"time\" + 0.061*\"many\" + 0.060*\"person\" + -0.054*\"plant\" + '\n",
      "  '-0.051*\"var\" + 0.050*\"war\"'),\n",
      " (2,\n",
      "  '0.296*\"football\" + 0.252*\"player\" + 0.251*\"american\" + 0.228*\"ball\" + '\n",
      "  '0.220*\"war\" + 0.209*\"play\" + 0.177*\"line\" + 0.172*\"team\" + 0.162*\"lincoln\" '\n",
      "  '+ -0.160*\"atheism\"'),\n",
      " (3,\n",
      "  '0.546*\"lincoln\" + 0.194*\"state\" + -0.187*\"football\" + 0.181*\"aristotle\" + '\n",
      "  '-0.181*\"player\" + 0.177*\"union\" + -0.140*\"ball\" + 0.140*\"war\" + '\n",
      "  '-0.119*\"play\" + 0.115*\"achille\"'),\n",
      " (4,\n",
      "  '-0.534*\"atheism\" + -0.373*\"god\" + -0.316*\"atheist\" + -0.176*\"lincoln\" + '\n",
      "  '-0.154*\"belief\" + 0.127*\"africa\" + -0.122*\"existence\" + -0.117*\"religion\" + '\n",
      "  '-0.102*\"deity\" + 0.102*\"hiv\"'),\n",
      " (5,\n",
      "  '0.286*\"lincoln\" + -0.222*\"aluminium\" + -0.206*\"island\" + 0.194*\"hiv\" + '\n",
      "  '0.176*\"abortion\" + -0.171*\"apple\" + 0.169*\"film\" + 0.159*\"aristotle\" + '\n",
      "  '-0.152*\"australia\" + -0.130*\"church\"'),\n",
      " (6,\n",
      "  '0.363*\"atom\" + 0.257*\"alkane\" + -0.201*\"hiv\" + 0.170*\"lincoln\" + '\n",
      "  '0.167*\"element\" + 0.163*\"aluminium\" + 0.159*\"electron\" + 0.156*\"court\" + '\n",
      "  '0.154*\"carbon\" + -0.143*\"africa\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "# Build the LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, id2word=dct, num_topics=7, decay=0.5)\n",
    "\n",
    "# View Topics\n",
    "pprint(lsi_model.print_topics(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "composite-destiny",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:37:16,942 : INFO : collecting all words and their counts\n",
      "2021-02-10 12:37:16,943 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-10 12:37:18,713 : INFO : collected 189074 word types from a corpus of 10000000 raw words and 1000 sentences\n",
      "2021-02-10 12:37:18,714 : INFO : Loading a fresh vocabulary\n",
      "2021-02-10 12:37:20,889 : INFO : effective_min_count=0 retains 189074 unique words (100% of original 189074, drops 0)\n",
      "2021-02-10 12:37:20,890 : INFO : effective_min_count=0 leaves 10000000 word corpus (100% of original 10000000, drops 0)\n",
      "2021-02-10 12:37:21,300 : INFO : deleting the raw counts dictionary of 189074 items\n",
      "2021-02-10 12:37:21,305 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2021-02-10 12:37:21,306 : INFO : downsampling leaves estimated 7563517 word corpus (75.6% of prior 10000000)\n",
      "2021-02-10 12:37:21,743 : INFO : estimated required memory for 189074 words and 100 dimensions: 245796200 bytes\n",
      "2021-02-10 12:37:21,743 : INFO : resetting layer weights\n",
      "2021-02-10 12:37:50,984 : INFO : training model with 12 workers on 189074 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-10 12:37:51,995 : INFO : EPOCH 1 - PROGRESS: at 19.40% examples, 1445241 words/s, in_qsize 23, out_qsize 0\n",
      "2021-02-10 12:37:53,005 : INFO : EPOCH 1 - PROGRESS: at 41.30% examples, 1540796 words/s, in_qsize 23, out_qsize 0\n",
      "2021-02-10 12:37:54,019 : INFO : EPOCH 1 - PROGRESS: at 61.30% examples, 1527880 words/s, in_qsize 19, out_qsize 4\n",
      "2021-02-10 12:37:55,023 : INFO : EPOCH 1 - PROGRESS: at 81.60% examples, 1528676 words/s, in_qsize 19, out_qsize 4\n",
      "2021-02-10 12:37:55,822 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:37:55,830 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:37:55,842 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:37:55,848 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:37:55,851 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:37:55,854 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:37:55,857 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:37:55,862 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:37:55,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:37:55,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:37:55,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:37:55,872 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:37:55,873 : INFO : EPOCH - 1 : training on 10000000 raw words (7564128 effective words) took 4.9s, 1548603 effective words/s\n",
      "2021-02-10 12:37:56,901 : INFO : EPOCH 2 - PROGRESS: at 22.40% examples, 1644086 words/s, in_qsize 20, out_qsize 3\n",
      "2021-02-10 12:37:57,903 : INFO : EPOCH 2 - PROGRESS: at 44.60% examples, 1658491 words/s, in_qsize 18, out_qsize 5\n",
      "2021-02-10 12:37:58,914 : INFO : EPOCH 2 - PROGRESS: at 66.40% examples, 1652656 words/s, in_qsize 22, out_qsize 1\n",
      "2021-02-10 12:37:59,941 : INFO : EPOCH 2 - PROGRESS: at 87.90% examples, 1635995 words/s, in_qsize 21, out_qsize 2\n",
      "2021-02-10 12:38:00,482 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:38:00,487 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:38:00,489 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:38:00,492 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:38:00,505 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:38:00,506 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:38:00,509 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:38:00,510 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:38:00,511 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:38:00,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:38:00,514 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:38:00,520 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:38:00,520 : INFO : EPOCH - 2 : training on 10000000 raw words (7565006 effective words) took 4.6s, 1629753 effective words/s\n",
      "2021-02-10 12:38:01,539 : INFO : EPOCH 3 - PROGRESS: at 19.50% examples, 1441593 words/s, in_qsize 20, out_qsize 3\n",
      "2021-02-10 12:38:02,545 : INFO : EPOCH 3 - PROGRESS: at 38.20% examples, 1421529 words/s, in_qsize 19, out_qsize 4\n",
      "2021-02-10 12:38:03,562 : INFO : EPOCH 3 - PROGRESS: at 60.00% examples, 1492355 words/s, in_qsize 22, out_qsize 4\n",
      "2021-02-10 12:38:04,574 : INFO : EPOCH 3 - PROGRESS: at 80.30% examples, 1498906 words/s, in_qsize 20, out_qsize 3\n",
      "2021-02-10 12:38:05,440 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:38:05,447 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:38:05,461 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:38:05,465 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:38:05,468 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:38:05,469 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:38:05,470 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:38:05,475 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:38:05,482 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:38:05,483 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:38:05,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:38:05,486 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:38:05,486 : INFO : EPOCH - 3 : training on 10000000 raw words (7563289 effective words) took 5.0s, 1524469 effective words/s\n",
      "2021-02-10 12:38:06,502 : INFO : EPOCH 4 - PROGRESS: at 21.20% examples, 1572300 words/s, in_qsize 23, out_qsize 0\n",
      "2021-02-10 12:38:07,516 : INFO : EPOCH 4 - PROGRESS: at 43.40% examples, 1611972 words/s, in_qsize 23, out_qsize 0\n",
      "2021-02-10 12:38:08,525 : INFO : EPOCH 4 - PROGRESS: at 66.00% examples, 1642874 words/s, in_qsize 20, out_qsize 4\n",
      "2021-02-10 12:38:09,531 : INFO : EPOCH 4 - PROGRESS: at 88.20% examples, 1649863 words/s, in_qsize 19, out_qsize 4\n",
      "2021-02-10 12:38:10,033 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:38:10,034 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:38:10,036 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:38:10,039 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:38:10,041 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:38:10,042 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:38:10,055 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:38:10,056 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:38:10,058 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:38:10,060 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:38:10,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:38:10,063 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:38:10,064 : INFO : EPOCH - 4 : training on 10000000 raw words (7564843 effective words) took 4.6s, 1653832 effective words/s\n",
      "2021-02-10 12:38:11,096 : INFO : EPOCH 5 - PROGRESS: at 21.50% examples, 1573846 words/s, in_qsize 21, out_qsize 2\n",
      "2021-02-10 12:38:12,109 : INFO : EPOCH 5 - PROGRESS: at 43.20% examples, 1594803 words/s, in_qsize 19, out_qsize 4\n",
      "2021-02-10 12:38:13,122 : INFO : EPOCH 5 - PROGRESS: at 65.40% examples, 1619101 words/s, in_qsize 20, out_qsize 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:38:14,135 : INFO : EPOCH 5 - PROGRESS: at 87.60% examples, 1629222 words/s, in_qsize 18, out_qsize 5\n",
      "2021-02-10 12:38:14,659 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:38:14,660 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:38:14,662 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:38:14,664 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:38:14,667 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:38:14,669 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:38:14,681 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:38:14,682 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:38:14,684 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:38:14,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:38:14,686 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:38:14,687 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:38:14,688 : INFO : EPOCH - 5 : training on 10000000 raw words (7564098 effective words) took 4.6s, 1638036 effective words/s\n",
      "2021-02-10 12:38:14,689 : INFO : training on a 50000000 raw words (37821364 effective words) took 23.7s, 1595558 effective words/s\n",
      "<ipython-input-26-88ed0fae9205>:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model['topic']\n",
      "<ipython-input-26-88ed0fae9205>:20: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar('topic')\n",
      "2021-02-10 12:38:14,690 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-02-10 12:38:14,827 : INFO : saving Word2Vec object under newmodel, separately None\n",
      "2021-02-10 12:38:14,828 : INFO : storing np array 'vectors' to newmodel.wv.vectors.npy\n",
      "2021-02-10 12:38:15,039 : INFO : not storing attribute vectors_norm\n",
      "2021-02-10 12:38:15,040 : INFO : storing np array 'syn1neg' to newmodel.trainables.syn1neg.npy\n",
      "2021-02-10 12:38:15,252 : INFO : not storing attribute cum_table\n",
      "2021-02-10 12:38:15,545 : INFO : saved newmodel\n",
      "2021-02-10 12:38:15,546 : INFO : loading Word2Vec object from newmodel\n",
      "2021-02-10 12:38:16,676 : INFO : loading wv recursively from newmodel.wv.* with mmap=None\n",
      "2021-02-10 12:38:16,676 : INFO : loading vectors from newmodel.wv.vectors.npy with mmap=None\n",
      "2021-02-10 12:38:16,731 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-02-10 12:38:16,732 : INFO : loading vocabulary recursively from newmodel.vocabulary.* with mmap=None\n",
      "2021-02-10 12:38:16,733 : INFO : loading trainables recursively from newmodel.trainables.* with mmap=None\n",
      "2021-02-10 12:38:16,733 : INFO : loading syn1neg from newmodel.trainables.syn1neg.npy with mmap=None\n",
      "2021-02-10 12:38:16,787 : INFO : setting ignored attribute cum_table to None\n",
      "2021-02-10 12:38:16,788 : INFO : loaded newmodel\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download dataset\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# Split the data into 2 parts. Part 2 will be used later to update the model\n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Get the word vector for given word\n",
    "model['topic']\n",
    "#> array([ 0.0512,  0.2555,  0.9393, ... ,-0.5669,  0.6737], dtype=float32)\n",
    "\n",
    "model.most_similar('topic')\n",
    "# Save and Load Model\n",
    "model.save('newmodel')\n",
    "model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "enormous-princeton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:51:34,879 : INFO : collecting all words and their counts\n",
      "2021-02-10 12:51:34,880 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-10 12:51:36,117 : INFO : collected 153347 word types from a corpus of 7005207 raw words and 701 sentences\n",
      "2021-02-10 12:51:36,118 : INFO : Updating model with new vocabulary\n",
      "2021-02-10 12:51:36,339 : INFO : New added 153347 unique words (50% of original 306694) and increased the count of 153347 pre-existing words (50% of original 306694)\n",
      "2021-02-10 12:51:37,082 : INFO : deleting the raw counts dictionary of 153347 items\n",
      "2021-02-10 12:51:37,085 : INFO : sample=0.001 downsamples 72 most-common words\n",
      "2021-02-10 12:51:37,086 : INFO : downsampling leaves estimated 10509051 word corpus (150.0% of prior 7005207)\n",
      "2021-02-10 12:51:37,446 : INFO : estimated required memory for 306694 words and 100 dimensions: 398702200 bytes\n",
      "2021-02-10 12:51:37,447 : INFO : updating layer weights\n",
      "<ipython-input-27-9d21da0c7aa6>:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)\n",
      "2021-02-10 12:51:47,638 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2021-02-10 12:51:47,640 : INFO : training model with 12 workers on 253854 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-10 12:51:48,660 : INFO : EPOCH 1 - PROGRESS: at 28.25% examples, 1471439 words/s, in_qsize 22, out_qsize 1\n",
      "2021-02-10 12:51:49,668 : INFO : EPOCH 1 - PROGRESS: at 58.77% examples, 1525679 words/s, in_qsize 18, out_qsize 5\n",
      "2021-02-10 12:51:50,682 : INFO : EPOCH 1 - PROGRESS: at 90.16% examples, 1561311 words/s, in_qsize 19, out_qsize 4\n",
      "2021-02-10 12:51:50,923 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:51:50,925 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:51:50,946 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:51:50,953 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:51:50,955 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:51:50,956 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:51:50,958 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:51:50,959 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:51:50,962 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:51:50,963 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:51:50,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:51:50,973 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:51:50,974 : INFO : EPOCH - 1 : training on 7005207 raw words (5254054 effective words) took 3.3s, 1578603 effective words/s\n",
      "2021-02-10 12:51:52,006 : INFO : EPOCH 2 - PROGRESS: at 31.67% examples, 1629583 words/s, in_qsize 20, out_qsize 3\n",
      "2021-02-10 12:51:53,007 : INFO : EPOCH 2 - PROGRESS: at 62.20% examples, 1610526 words/s, in_qsize 23, out_qsize 0\n",
      "2021-02-10 12:51:54,012 : INFO : EPOCH 2 - PROGRESS: at 93.30% examples, 1616213 words/s, in_qsize 20, out_qsize 3\n",
      "2021-02-10 12:51:54,176 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:51:54,181 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:51:54,183 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:51:54,185 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:51:54,186 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:51:54,188 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:51:54,195 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:51:54,196 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:51:54,201 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:51:54,202 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:51:54,204 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:51:54,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:51:54,206 : INFO : EPOCH - 2 : training on 7005207 raw words (5253235 effective words) took 3.2s, 1627553 effective words/s\n",
      "2021-02-10 12:51:55,221 : INFO : EPOCH 3 - PROGRESS: at 32.10% examples, 1678392 words/s, in_qsize 20, out_qsize 3\n",
      "2021-02-10 12:51:56,227 : INFO : EPOCH 3 - PROGRESS: at 64.48% examples, 1679309 words/s, in_qsize 20, out_qsize 4\n",
      "2021-02-10 12:51:57,234 : INFO : EPOCH 3 - PROGRESS: at 96.86% examples, 1683744 words/s, in_qsize 22, out_qsize 0\n",
      "2021-02-10 12:51:57,269 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:51:57,295 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:51:57,297 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:51:57,299 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:51:57,300 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:51:57,301 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:51:57,303 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:51:57,309 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:51:57,314 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:51:57,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:51:57,316 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:51:57,317 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:51:57,318 : INFO : EPOCH - 3 : training on 7005207 raw words (5255791 effective words) took 3.1s, 1691034 effective words/s\n",
      "2021-02-10 12:51:58,327 : INFO : EPOCH 4 - PROGRESS: at 31.67% examples, 1672576 words/s, in_qsize 20, out_qsize 3\n",
      "2021-02-10 12:51:59,327 : INFO : EPOCH 4 - PROGRESS: at 64.34% examples, 1688669 words/s, in_qsize 23, out_qsize 4\n",
      "2021-02-10 12:52:00,333 : INFO : EPOCH 4 - PROGRESS: at 95.72% examples, 1672743 words/s, in_qsize 17, out_qsize 6\n",
      "2021-02-10 12:52:00,415 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:52:00,421 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-10 12:52:00,422 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:52:00,425 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:52:00,427 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:52:00,438 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:52:00,439 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:52:00,442 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:52:00,444 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:52:00,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:52:00,445 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:52:00,447 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:52:00,448 : INFO : EPOCH - 4 : training on 7005207 raw words (5253997 effective words) took 3.1s, 1682960 effective words/s\n",
      "2021-02-10 12:52:01,456 : INFO : EPOCH 5 - PROGRESS: at 29.67% examples, 1564743 words/s, in_qsize 24, out_qsize 0\n",
      "2021-02-10 12:52:02,459 : INFO : EPOCH 5 - PROGRESS: at 59.34% examples, 1553312 words/s, in_qsize 23, out_qsize 0\n",
      "2021-02-10 12:52:03,470 : INFO : EPOCH 5 - PROGRESS: at 90.44% examples, 1576631 words/s, in_qsize 18, out_qsize 5\n",
      "2021-02-10 12:52:03,709 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-10 12:52:03,714 : INFO : worker thread finished; awaiting finish of 10 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:52:03,724 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-10 12:52:03,726 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-10 12:52:03,729 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-10 12:52:03,731 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-10 12:52:03,732 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-10 12:52:03,733 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-10 12:52:03,739 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-10 12:52:03,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-10 12:52:03,745 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-10 12:52:03,747 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-10 12:52:03,748 : INFO : EPOCH - 5 : training on 7005207 raw words (5253850 effective words) took 3.3s, 1595030 effective words/s\n",
      "2021-02-10 12:52:03,749 : INFO : training on a 35026035 raw words (26270927 effective words) took 16.1s, 1630952 effective words/s\n",
      "<ipython-input-27-9d21da0c7aa6>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model['topic']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.10153081,  0.1926781 ,  0.32744864, -0.01678169, -0.24205594,\n",
       "       -1.7215012 ,  1.5836352 , -0.13802871, -1.0151654 , -0.67926466,\n",
       "        0.99595296,  0.1391699 ,  0.33435762,  3.0574293 , -0.1753422 ,\n",
       "        1.6304696 ,  0.7603253 ,  0.3986856 , -1.5905765 ,  2.3999097 ,\n",
       "       -0.26273668,  1.4957714 , -0.3528074 ,  1.1807108 ,  0.99626297,\n",
       "       -1.2842723 , -1.7719195 , -0.43413588, -0.34358102, -1.4486936 ,\n",
       "       -1.2305747 ,  0.42596608,  0.27028072,  1.1097169 , -0.6121068 ,\n",
       "        0.2724385 ,  0.41296592,  0.74607074, -0.97312564, -1.4279096 ,\n",
       "       -0.40815052,  0.28389895,  0.23084855,  1.4811028 , -0.08197882,\n",
       "        1.3253828 ,  0.6860957 ,  0.5596175 ,  0.645516  , -0.04176502,\n",
       "        0.0863863 , -1.2047492 , -0.37585747,  0.52356535, -0.13275096,\n",
       "       -1.0186473 ,  0.03767383,  1.7454988 , -0.79560745,  0.21469817,\n",
       "        0.8225354 , -0.46741393,  0.70313364,  0.42461437, -0.22032373,\n",
       "        0.06962129, -0.33938822,  0.3546789 ,  0.16525197,  0.43748835,\n",
       "        0.01580582, -0.28305238, -0.2200054 ,  1.8256994 ,  0.55650455,\n",
       "       -0.2112485 , -0.16424309,  0.8526053 , -0.9355941 , -0.0081001 ,\n",
       "       -0.56220317,  0.6104377 ,  0.34056392,  1.0899386 , -0.6073346 ,\n",
       "        0.07253884,  0.43696797,  0.95632786, -1.392707  ,  1.0917585 ,\n",
       "        0.0900503 ,  0.00507478,  1.9827389 , -0.2874854 ,  0.9115365 ,\n",
       "        0.4910306 ,  0.5138048 ,  1.9699689 ,  0.03094473,  1.9076037 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the model with new data.\n",
    "model.build_vocab(data_part2, update=True)\n",
    "model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "typical-candy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 100.0% 958.2/958.4MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:54:25,308 : INFO : fasttext-wiki-news-subwords-300 downloaded\n",
      "2021-02-10 12:54:25,315 : INFO : loading projection weights from /Users/imayak/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\n",
      "2021-02-10 12:57:06,471 : INFO : loaded (999999, 300) matrix from /Users/imayak/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\n",
      "2021-02-10 12:57:06,536 : INFO : loading projection weights from /Users/imayak/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2021-02-10 12:57:55,235 : INFO : loaded (3000000, 300) matrix from /Users/imayak/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 12:58:49,839 : INFO : glove-wiki-gigaword-300 downloaded\n",
      "2021-02-10 12:58:49,843 : INFO : loading projection weights from /Users/imayak/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "2021-02-10 12:59:54,888 : INFO : loaded (400000, 300) matrix from /Users/imayak/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "2021-02-10 12:59:54,893 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('supporting', 0.6251285076141357),\n",
       " ('suport', 0.6071150302886963),\n",
       " ('suppport', 0.6053199768066406),\n",
       " ('Support', 0.6044273376464844),\n",
       " ('supported', 0.6009396910667419),\n",
       " ('backing', 0.6007589101791382),\n",
       " ('supports', 0.5269277095794678),\n",
       " ('assistance', 0.5207138061523438),\n",
       " ('sup_port', 0.5192490220069885),\n",
       " ('supportive', 0.5110025405883789)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the models\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "glove_model300 = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "# Get word embeddings\n",
    "word2vec_model300.most_similar('support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "perceived-stephen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wireless_broadband', 0.6713308095932007),\n",
       " ('telecommunications', 0.6429787874221802),\n",
       " ('NetInformer_leading', 0.6375865936279297),\n",
       " ('GigaBeam_revolutionary', 0.6346628665924072),\n",
       " ('Airband_fixed', 0.6342990398406982),\n",
       " ('ISDN_ADSL', 0.6333105564117432),\n",
       " ('servicesand', 0.629921555519104),\n",
       " ('Apprion_delivers', 0.628883957862854),\n",
       " ('Aruba_wireless_LAN', 0.6278460025787354),\n",
       " ('IP_Roamer', 0.6219089031219482)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model300.most_similar(['wireless', 'services'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "located-kruger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-88382026bdf0>:9: DeprecationWarning: Call to deprecated `similarity_matrix` (Method will be removed in 4.0.0, use gensim.models.keyedvectors.WordEmbeddingSimilarityIndex instead).\n",
      "  similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
      "2021-02-10 15:37:51,293 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7ff2e7c782e0>\n",
      "2021-02-10 15:37:51,294 : INFO : iterating over columns in dictionary order\n",
      "2021-02-10 15:37:51,300 : INFO : PROGRESS: at 4.35% columns (1 / 23, 4.347826% density, 4.347826% projected density)\n",
      "2021-02-10 15:37:52,865 : INFO : constructed a sparse term similarity matrix with 9.640832% density\n",
      "2021-02-10 15:37:52,867 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-10 15:37:52,867 : INFO : built Dictionary(23 unique tokens: ['It', 'always', 'and', 'are', 'during']...) from 3 documents (total 27 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3679225482343939\n",
      "0.3408726649549052\n",
      "0.12996676575362087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-88382026bdf0>:21: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  print(softcossim(sent_1, sent_2, similarity_matrix))\n",
      "<ipython-input-35-88382026bdf0>:23: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  print(softcossim(sent_1, sent_3, similarity_matrix))\n",
      "<ipython-input-35-88382026bdf0>:25: DeprecationWarning: Call to deprecated `softcossim` (Function will be removed in 4.0.0, use gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product instead).\n",
      "  print(softcossim(sent_2, sent_3, similarity_matrix))\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import softcossim\n",
    "from gensim import corpora\n",
    "\n",
    "sent_1 = 'It always snows during winter and some are worse'.split()\n",
    "sent_2 = 'When it snows during spring, it surprises many'.split()\n",
    "sent_3 = 'During sports season, snow and bad weather does not bother '.split()\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# Prepare a dictionary and a corpus.\n",
    "documents = [sent_1, sent_2, sent_3]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(sent_1)\n",
    "sent_2 = dictionary.doc2bow(sent_2)\n",
    "sent_3 = dictionary.doc2bow(sent_3)\n",
    "\n",
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "\n",
    "print(softcossim(sent_1, sent_3, similarity_matrix))\n",
    "\n",
    "print(softcossim(sent_2, sent_3, similarity_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "binding-maximum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imayak/exper/python_nlp_venv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beetroot\n",
      "0.22957539558410645\n",
      "[0.22957546 0.465837   0.547001  ]\n",
      "[0.77042454 0.534163   0.45299897 0.76572543]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 15:41:11,155 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prince', 'queen', 'monarch']\n",
      "[('king-', 0.7838029265403748), ('boy-king', 0.7704817652702332), ('queen', 0.7704246044158936), ('prince', 0.7700966596603394), ('kings', 0.7668929696083069)]\n",
      "[('queen', 0.8168227076530457), ('prince', 0.809830367565155), ('monarch', 0.7949802875518799), ('kingdom', 0.7895625829696655), ('throne', 0.7803236842155457)]\n"
     ]
    }
   ],
   "source": [
    "# Which word from the given list doesn't go with the others?\n",
    "print(fasttext_model300.doesnt_match(['india', 'australia', 'pakistan', 'china', 'beetroot']))\n",
    "\n",
    "# Compute cosine distance between two words.\n",
    "print(fasttext_model300.distance('king', 'queen'))\n",
    "\n",
    "# Compute cosine distances from given word or vector to all words in `other_words`.\n",
    "print(fasttext_model300.distances('king', ['queen', 'man', 'woman']))\n",
    "\n",
    "# Compute cosine similarities\n",
    "print(fasttext_model300.cosine_similarities(fasttext_model300['king'], \n",
    "                                            vectors_all=(fasttext_model300['queen'], \n",
    "                                                        fasttext_model300['man'], \n",
    "                                                        fasttext_model300['woman'],\n",
    "                                                        fasttext_model300['queen'] + fasttext_model300['man'])))  \n",
    "# Get the words closer to w1 than w2\n",
    "print(glove_model300.words_closer_than(w1='king', w2='kingdom'))\n",
    "\n",
    "# Find the top-N most similar words.\n",
    "print(fasttext_model300.most_similar(positive='king', negative=None, topn=5, restrict_vocab=None, indexer=None))\n",
    "# Find the top-N most similar words, using the multiplicative combination objective,\n",
    "\n",
    "print(glove_model300.most_similar_cosmul(positive='king', negative=None, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_nlp_venv",
   "language": "python",
   "name": "python_nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
